import spacy
import re
from spacy.matcher import PhraseMatcher
import pymorphy3




#–•—Ä–µ–Ω—å –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –≤ –µ–¥ —á–∏—Å–ª–æ –∏–º –ø–∞–¥–µ–∂–∞ —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã (–≤—Å–ø–æ–º–∏–Ω–∞–µ–º —É—Ä–æ–∫–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ 4 –∫–ª–∞—Å—Å)
def to_nominative_singular(word):
    word = word.split(" ")
    morph = pymorphy3.MorphAnalyzer() # –£–º–Ω—ã–π —á–µ–ª –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–µ—Ç –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –ø–∞–¥–µ–∂—ã
    for i, w in enumerate(word):
        word[i] = morph.parse(w)[0].normal_form.capitalize() # –ü–µ—Ä–µ–≤–æ–¥–∏–º
    return " ".join(word)

# NLP –º–æ–¥–µ–ª–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
nlp_en = spacy.load("en_core_web_sm")
nlp_ru = spacy.load("ru_core_news_sm")

# –•—Ä–µ–Ω—å —á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ –∏–∫–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ
work_form_matcher_en = PhraseMatcher(nlp_en.vocab)
work_form_matcher_ru = PhraseMatcher(nlp_ru.vocab)

work_forms = ["remote", "hybrid", "office", "—É–¥–∞–ª—ë–Ω–∫–∞", "–æ—Ñ–∏—Å", "–≥–∏–±—Ä–∏–¥"]
work_forms += [i.capitalize() for i in work_forms]

# Add patterns to both matchers
for matcher, nlp in [(work_form_matcher_en, nlp_en), (work_form_matcher_ru, nlp_ru)]:
    patterns = [nlp(text) for text in work_forms]
    matcher.add("WORK_FORM", patterns)


# PRE-BUILT VACANCY MATCHERS (FIXED)
vacancy_matcher_en = PhraseMatcher(nlp_en.vocab)
vacancy_matcher_ru = PhraseMatcher(nlp_ru.vocab)

# Enhanced vacancy keywords (added "—Å—Ç–∞–∂—ë—Ä")
vacancy_keywords = {
    'en': ["hiring", "apply now", "job opening", "vacancy", "position", "opportunity"],
    'ru': ["–∏—â–µ–º", "—Ç—Ä–µ–±—É–µ—Ç—Å—è", "–≤–∞–∫–∞–Ω—Å–∏—è", "–Ω–∞–±–æ—Ä", "—Å—Ç–∞–∂—ë—Ä", "—Ä–∞–±–æ—Ç–∞", "—Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"]
}

# Create patterns for vacancy detection
patterns_en = [nlp_en(text) for text in vacancy_keywords['en']]
patterns_ru = [nlp_ru(text) for text in vacancy_keywords['ru']]
vacancy_matcher_en.add("VACANCY", patterns_en)
vacancy_matcher_ru.add("VACANCY", patterns_ru)

# Add custom patterns for job titles
job_title_patterns = [
    {"label": "OCCUPATION", "pattern": [{"LOWER": "—Å—Ç–∞–∂—ë—Ä"}, {"LOWER": "data"}, {"LOWER": "science"}]},
    {"label": "OCCUPATION", "pattern": [{"LOWER": "data"}, {"LOWER": "scientist"}]},
    {"label": "OCCUPATION", "pattern": [{"LOWER": "–º–µ–Ω–µ–¥–∂–µ—Ä"}]},
    {"label": "OCCUPATION", "pattern": [{"LOWER": "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫"}]}
]

# Add patterns to both pipelines
for nlp in [nlp_en, nlp_ru]:
    ruler = nlp.add_pipe("entity_ruler")
    ruler.add_patterns(job_title_patterns)



def is_vacancy(doc):
    """Check if document contains vacancy keywords"""
    if doc.lang_ == "en":
        matches = vacancy_matcher_en(doc)
    else:  # Assume Russian
        matches = vacancy_matcher_ru(doc)
    return len(matches) > 0


def extract_job_title(doc):
    # First look for OCCUPATION entities
    text = doc.text
    for ent in doc.ents:
        if ent.label_ == "OCCUPATION":
            return ent.text.capitalize()

    # Look for keywords in the text
    job_keywords = ["developer", "–º–µ–Ω–µ–¥–∂–µ—Ä", "—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç", "—Å—Ç–∞–∂—ë—Ä", "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫", "scientist", "–º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥", "–¥–∏–∑–∞–π–Ω–µ—Ä"]
    for token in doc:
        if token.text.lower() in job_keywords:
            # Get surrounding text (3 words before and after)
            start = max(0, token.i)
            end = min(len(doc), token.i + 4)
            return doc[start:end].text.capitalize()

    if doc.lang_ == "en":
        for chunk in doc.noun_chunks:
            return chunk.text.capitalize()
    job_keywords = {
        "–°—Ç–∞–∂—ë—Ä": ["—Å—Ç–∞–∂–∏—Ä–æ–≤–∫", "c—Ç–∞–∂–∏—Ä–æ–≤–∫", "—Å—Ç–∞–∂—ë—Ä", "c—Ç–∞–∂—ë—Ä"],
        "developer": ["developer"],
        "–º–µ–Ω–µ–¥–∂–µ—Ä": ["–º–µ–Ω–µ–¥–∂–µ—Ä"],
        "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫": ["—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫"],
        "scientist": ["scientist"],
        "–º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥": ["–º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥"],
        "–¥–∏–∑–∞–π–Ω–µ—Ä": ["–¥–∏–∑–∞–π–Ω–µ—Ä"]
    }

    text_lower = text.lower()

    for job, keywords in job_keywords.items():
        for keyword in keywords:
            if keyword in text_lower:
                return job.capitalize()
    return "–ù–µ —É–∫–∞–∑–∞–Ω–æ"


def parse_vacancy(text):
    text = text.replace("*", "")
    doc_ru = nlp_ru(text)
    result = {
        "is_vacancy": is_vacancy(doc_ru),
        "job_title": extract_job_title(doc_ru),
        "description": text
    }
    return result


#–¢–µ—Å—Ç
if __name__ == '__main__':
    text = """üì£ **–°—Ç–∞–∂—ë—Ä Data Science –≤ Ozon Tech**

    –ü—Ä–æ–µ–∫—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–æ–Ω–∞ –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤ –≤–æ –≤—Ä–µ–º—è —Ä–∞—Å–ø—Ä–æ–¥–∞–∂

    **üìå –ó–∞–¥–∞—á–∏: **
    - —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    - –æ–±—É—á–µ–Ω–∏–µ CV-–º–æ–¥–µ–ª–µ–π, –ø—Ä–æ–¥—É–º—ã–≤–∞–Ω–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑, –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    - —Ä–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ - —Å–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞, –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á–µ–π
    - –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π, –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—Å—è —Å –º–µ—Ç–æ–¥–∞–º–∏ –¥–µ–ø–ª–æ—è –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–æ–¥
    - –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –±–∏–∑–Ω–µ—Å-–∑–∞–∫–∞–∑—á–∏–∫–∞–º–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥ –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –≤ –ø–ª–æ—Å–∫–æ—Å—Ç—å ML

    –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã –æ—Ç 20 –ª–µ—Ç
    –ó–∞—Ä–ø–ª–∞—Ç–∞ –æ—Ç 0 –¥–æ 10 —Ä—É–±–ª–µ–π –≤ –º–µ—Å—è—Ü
    –ò–Ω–Ω–æ–ø–æ–ª–∏—Å
    –æ—Ñ–∏—Å
    
[üëâüèº **–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ –æ —Å—Ç–∞–∂–∏—Ä–æ–≤–∫–µ**](https://job.ozon.ru/vacancy/121749776?__rr=1&abt_att=1)"""

    print(parse_vacancy(text))